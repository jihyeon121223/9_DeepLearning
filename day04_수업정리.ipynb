{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e610e4-1828-4001-bbc5-8388d1bd1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Activation\n",
    "def sigmoid(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x<=0,0,x)\n",
    "\n",
    "def softmax(x): #원래 행렬의 모양대로 출력시키기 위해 한값(소프트맥스)만 곱함\n",
    "    x = x - np.max(x,axis=1).reshape(-1,1)\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502bbc10-81fe-4979-9aa3-6477169d7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loss function\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-5\n",
    "    return -np.sum(t*np.log(y+delta))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff2ff77-c633-44d6-af1e-5313ea02e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 differ function\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    if x.ndim == 2:\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                fx = f(x[i,j])\n",
    "                tmp_val = x[i,j]\n",
    "                x[i,j] = tmp_val + h\n",
    "                fxh = f(x[i,j])\n",
    "                grad[i,j] = (fxh - fx)/h\n",
    "                x[i,j] = tmp_val\n",
    "    else:\n",
    "        for i in range(x.size):\n",
    "            fx = f(x[i])\n",
    "            tmp_val = x[i]\n",
    "            x[i] = tmp_val + h\n",
    "            fxh = f(x[i])\n",
    "            grad[i] = (fxh - fx)/h\n",
    "            x[i] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db24c3fd-b0ac-47d6-9bc7-dfe6ce4590cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w):\n",
    "    return softmax(np.dot(x,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f019af6a-15b1-4a37-910d-dba040176fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. class\n",
    "class OneLayer:\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.W = {}\n",
    "        self.W['W1'] = np.random.randn(input_size,output_size)\n",
    "        self.W['b'] = np.random.randn(output_size)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        W1, b = self.W['W1'], self.W['b']\n",
    "        pred = softmax(np.dot(x,W1) + b)\n",
    "        return pred\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        f = lambda W: cross_entropy_error(y,t)\n",
    "        grad = {}\n",
    "        grad['W1'] = numerical_gradient(f,self.W['W1'])\n",
    "        grad['b'] = numerical_gradient(f, self.W['b'])\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        acc = np.sum(np.argmax(y,axis=1) == np.argmax(t,axis=1))/y.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    def fit(self,x,t,epochs=1000,lr=1e-3,verbos=1):\n",
    "        for epoch in range(epochs):\n",
    "            self.W['W1'] = self.W['W1'] - lr*self.numerical_gradient(x,t)['W1']\n",
    "            self.W['b'] -= lr*self.numerical_gradient(x,t)['b']\n",
    "            if verbos == 1:\n",
    "                print(\"=========== loss \",self.loss(x,t), \"======== acc \",self.accuracy(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b9ff63-28b9-4ef6-8aed-c1387d599e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일차원 학습\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "X = load_wine()['data']\n",
    "y = load_wine()['target']\n",
    "t = np.zeros((y.size,np.unique(y).size))\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = t.shape[1]\n",
    "model = OneLayer(input_size=input_size, output_size=output_size)\n",
    "\n",
    "model.fit(X,t,verbos=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722ac6a3-50eb-4abd-833a-3395a70a5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet: \n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        self.W = {}\n",
    "        self.W['W1'] = np.random.randn(input_size,hidden_size)\n",
    "        self.W['b1'] = np.random.randn(hidden_size)\n",
    "        self.W['W2'] = np.random.randn(hidden_size,output_size)\n",
    "        self.W['b2'] = np.random.randn(output_size)\n",
    "        self.loss_val = []\n",
    "    \n",
    "    def predict(self,x):\n",
    "        W1 = self.W['W1']\n",
    "        W2 = self.W['W2']\n",
    "        b1 = self.W['b1']\n",
    "        b2 = self.W['b2']\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1 # 출력값\n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        out = softmax(a2)\n",
    "        return out\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss\n",
    "\n",
    "    def numerical_gradient(self,x,t):\n",
    "        f = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grad = {}\n",
    "        grad['W1'] = numerical_gradient(f, self.W['W1'])\n",
    "        grad['b1'] = numerical_gradient(f, self.W['b1'])\n",
    "        grad['W2'] = numerical_gradient(f, self.W['W2'])\n",
    "        grad['b2'] = numerical_gradient(f, self.W['b2'])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        acc = sum(y == t)/x.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    def train(self,epochs,lr,x,t):\n",
    "        for epoch in range(epochs):\n",
    "            grad = self.numerical_gradient(x,t)\n",
    "            for key in grad.keys():\n",
    "                self.W[key] -= lr*grad[key]\n",
    "            self.loss_val.append(self.loss(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2771d227-d11f-489a-8ee7-5a9179868970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ThreeLayerNet:\n",
    "    def __init__(self,input_size,hidden_size1,hidden_size2,output_size):\n",
    "        self.W = {}\n",
    "        self.W['W1'] = np.random.randn(input_size,hidden_size1)\n",
    "        self.W['b1'] = np.random.randn(hidden_size1)\n",
    "        self.W['W2'] = np.random.randn(hidden_size1,hidden_size2)\n",
    "        self.W['b2'] = np.random.randn(hidden_size2)\n",
    "        self.W['W3'] = np.random.randn(hidden_size2,output_size)\n",
    "        self.W['b3'] = np.random.randn(output_size)\n",
    "        self.loss_val = []\n",
    "    \n",
    "    def predict(self,x):\n",
    "        W1 = self.W['W1']\n",
    "        W2 = self.W['W2']\n",
    "        W3 = self.W['W3']\n",
    "        b1 = self.W['b1']\n",
    "        b2 = self.W['b2']\n",
    "        b3 = self.W['b3']\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1 \n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        z2 = relu(a2)\n",
    "        a3 = np.dot(z2,W3) + b3\n",
    "        out = softmax(a3)\n",
    "        return out\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss\n",
    "\n",
    "    def numerical_gradient(self,x,t):\n",
    "        f = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grad = {}\n",
    "        grad['W1'] = numerical_gradient(f, self.W['W1'])\n",
    "        grad['b1'] = numerical_gradient(f, self.W['b1'])\n",
    "        grad['W2'] = numerical_gradient(f, self.W['W2'])\n",
    "        grad['b2'] = numerical_gradient(f, self.W['b2'])\n",
    "        grad['W3'] = numerical_gradient(f, self.W['W3'])\n",
    "        grad['b3'] = numerical_gradient(f, self.W['b3'])\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "        acc = sum(y == t)/x.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    def fit(self,epochs,lr,x,t):\n",
    "        for epoch in range(epochs):\n",
    "            grad = self.numerical_gradient(x,t)\n",
    "            for key in grad.keys():\n",
    "                self.W[key] -= lr*grad[key]\n",
    "            self.loss_val.append(self.loss(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1d6fff-8d80-4504-ad27-00bd2106a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one(x):\n",
    "    if x.ndim == 2:\n",
    "        x = np.ravel(x.values)\n",
    "    t = np.zeros((x.size,np.unique(x).size))\n",
    "    for i in range(t.shape[0]):\n",
    "        t[i,x[i]] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcaf7a57-0e09-45bc-a59e-9c37daade693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23b9db02220>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3차원 학습\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X = load_digits()['data']\n",
    "y = load_digits()['target']\n",
    "t = make_one(y)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = t.shape[1]\n",
    "hidden_size1 = 10\n",
    "hidden_size2 = 5\n",
    "\n",
    "model = ThreeLayerNet(input_size,hidden_size1,hidden_size2,output_size)\n",
    "model.fit(100,1e-3,X,t)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(model.loss_val)),model.loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414f08d5-08de-4dab-bc4c-fec73fb8f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다차원 학습\n",
    "class MultiLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        hidden_size.append(output_size)\n",
    "        self.W = {}\n",
    "        self.W = {}\n",
    "        self.W['Input'] = np.random.randn(input_size,hidden_size[0])\n",
    "        self.W['Input_b'] = np.random.randn(hidden_size[0])\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            w = 'W'+str(i)\n",
    "            b = 'b'+str(i)\n",
    "            self.W[w] = np.random.randn(hidden_size[i],hidden_size[i+1])\n",
    "            self.W[b] = np.random.randn(hidden_size[i+1])      \n",
    "    \n",
    "    def predict(self,x):\n",
    "        j = 0\n",
    "        for i in range(len(self.W)):\n",
    "            if j % 2 == 0 and i < (len(self.W)-1):\n",
    "                x = relu(np.dot(x,self.W[list(self.W.keys())[i]]) + self.W[list(self.W.keys())[i+1]])\n",
    "            elif j % 2 == 0 and i >= (len(self.W)-1):\n",
    "                x = (np.dot(x,self.W[list(self.W.keys())[i]]) + self.W[list(self.W.keys())[i+1]])\n",
    "            j += 1\n",
    "        return softmax(x)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        f = lambda W: self.loss(x,t)\n",
    "        grads = {}\n",
    "        for key in self.W.keys():\n",
    "            grads[key] = numerical_gradient(f,self.W[key])\n",
    "        return grads\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = np.argmax(self.predict(x),axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        acc = np.sum(y==t)/y.size\n",
    "        return acc\n",
    "    \n",
    "    def fit(self,epochs,lr,x,t,verbos=1):\n",
    "        for epoch in range(epochs):\n",
    "            for key in self.W.keys():\n",
    "                self.W[key] -= lr*self.numerical_gradient(x,t)[key]\n",
    "            if verbos == 1:\n",
    "                print(epoch,\":epoch============== accuracy: \",self.accuracy(x,t),\"==========loss :\", self.loss(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410d859-1b12-4ba0-b286-c55203f718ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
