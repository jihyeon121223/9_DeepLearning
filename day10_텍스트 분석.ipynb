{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba6e572-c4c4-4a8f-8db8-8f28996912e0",
   "metadata": {},
   "source": [
    "# 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1589bb09-0483-4ca1-8bb1-880648cb3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66981500-ce45-405f-a290-3448a5437a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "17473536/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = imdb.load_data() #_raw :: 원본데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d93833-e09a-44e3-8859-ac0995012e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "1654784/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dir(imdb)\n",
    "word_index = imdb.get_word_index() #워드 인덱스 순서\n",
    "index_word = {v:k for k,v in word_index.items()} #인덱스 워드 순서로 바꾸기, 딕셔너리니깐 items()\n",
    "#v: 단어를 통해 키를 찾는거\n",
    "#k: 키를 통해 단어를 찾는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71148e69-917d-4012-806b-db0e9e4108aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#나의 워드 인덱스로 만들기\n",
    "#stop word :: 각종기호, 숫자, 구령호를 처리해야함(키에러 날리기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0caa4202-e8d8-496d-8010-6cf1fdba4952",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index #키에러 확인\n",
    "max(word_index.values()) #큰값확인 #88584\n",
    "min(word_index.values()) #작은값확인 #1\n",
    "X_train_raw[0] #번호확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f266e3-71e9-4cfd-95c9-e62f553191e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어별로 나오는거 연결해서 문장형태로 #방법1\n",
    "def make_sentence(x):\n",
    "    doc = []\n",
    "    for i in x:\n",
    "        tmp = []\n",
    "        for j in range(len(i)):\n",
    "            if i[j] <= 88584: #이거 없으면 88584보다 큰 값을 제거해줘야함(전처리)\n",
    "                tmp.append(index_word[i[j]])\n",
    "                sentence = ' '.join(tmp)\n",
    "        doc.append(sentence)\n",
    "    return pd.DataFrame(doc,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425beba7-8443-4246-a1a0-31ff3ae84cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_sentence(X_train_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fcd3d2-7365-4242-b8db-f1d6b618e038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    text\n",
       " 0      the as you with out themselves powerful lets l...\n",
       " 1      the thought solid thought senator do making to...\n",
       " 2      the as there in at by br of sure many br of pr...\n",
       " 3      the of bernadette mon they halfway of identity...\n",
       " 4      the sure themes br only acting i i was favouri...\n",
       " ...                                                  ...\n",
       " 24995  the as it is ludicrous on not rape br program ...\n",
       " 24996  the slaughter susan effects is following like ...\n",
       " 24997  the this is anything tv tormented it is genera...\n",
       " 24998  the bar reverse me we endearing was song deep ...\n",
       " 24999  the movie is thought completely br of i've you...\n",
       " \n",
       " [25000 rows x 1 columns],\n",
       " array([1, 0, 0, ..., 0, 1, 0], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train_raw #그냥 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1bfec2-4c75-4eb0-a104-43ccd8d63042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the as you with out themselves powerful lets l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the thought solid thought senator do making to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the as there in at by br of sure many br of pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the of bernadette mon they halfway of identity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the sure themes br only acting i i was favouri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>the as it is ludicrous on not rape br program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>the slaughter susan effects is following like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>the this is anything tv tormented it is genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>the bar reverse me we endearing was song deep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>the movie is thought completely br of i've you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      the as you with out themselves powerful lets l...\n",
       "1      the thought solid thought senator do making to...\n",
       "2      the as there in at by br of sure many br of pr...\n",
       "3      the of bernadette mon they halfway of identity...\n",
       "4      the sure themes br only acting i i was favouri...\n",
       "...                                                  ...\n",
       "24995  the as it is ludicrous on not rape br program ...\n",
       "24996  the slaughter susan effects is following like ...\n",
       "24997  the this is anything tv tormented it is genera...\n",
       "24998  the bar reverse me we endearing was song deep ...\n",
       "24999  the movie is thought completely br of i've you...\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train #그냥 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9729761-c540-4752-8bee-05dfbdb72df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#단어별로 나오는거 연결해서 문장형태로 #방법2\n",
    "#88584보다 큰 값을 제거하기(전처리)\n",
    "np.array(X_train_raw[0])[np.array(X_train_raw[0])<=max(word_index.values())]\n",
    "\n",
    "def make_sentence(x):\n",
    "    # x = np.array[np.array(X_train_raw[0])<=88584] #방법1\n",
    "    doc = []\n",
    "    for i in x:\n",
    "        i = np.array(i)[np.array(i)<=max(word_index.values())] #방법2\n",
    "        tmp = []\n",
    "        for j in range(len(i)):\n",
    "            tmp.append(index_word[i[j]])\n",
    "            sentence = ' '.join(tmp)\n",
    "        doc.append(sentence)\n",
    "    return pd.DataFrame(doc,columns=['text'])\n",
    "\n",
    "## 최대길이 찾기 by정승\n",
    "# max_len = max(max([max(sentence) for sentence in X_train]),max([max(sentence) for sentence in X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8c7694-a0a6-4a62-b44e-ba7f3e956990",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_sentence(X_train_raw)\n",
    "X_test = make_sentence(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad86103-c1bf-4115-a96c-7481c49af363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test 합쳐 unique 정수인덱스, 단어인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b26358-be42-4d86-91ad-90bf444b5701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_train,X_test])\n",
    "y = np.concatenate([y_train_raw,y_test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60cf2617-d3e1-4aca-a339-052adba41a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6a4831-2c3f-430d-bc3e-0413c6298e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [the, as, you, with, out, themselves, powerful...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X #그냥 확인\n",
    "X.reset_index(inplace=True) #얘 안먹히네 \n",
    "X.text[0]\n",
    "len(X.text[0])\n",
    "X.text[0].split()\n",
    "X.text[0:1].str.split() #시리즈 값\n",
    "# X.text.apply(func) #인수가 없이 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b65b326-a769-4f2a-8d3c-a64e9b4fb8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11738013"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = []\n",
    "for i in X.text:\n",
    "    # print(i) #하나씩 떨어지면 문제 있는거임(알파벳)\n",
    "    tmp = i.split() #단어리스트 #단어 하나씩 떼서 나중에 숫자로 만들거라 처리중\n",
    "    full_text.extend(tmp)\n",
    "\n",
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2f2da7-441a-4a1d-a6c7-43ffea86def9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_word = list(set(full_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a992407-8b03-403b-b10b-390a876da719",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'oodishon',\n",
       " 2: 'witted',\n",
       " 3: 'croc',\n",
       " 4: 'slack',\n",
       " 5: 'hermes',\n",
       " 6: \"gackt's\",\n",
       " 7: 'intriquing',\n",
       " 8: 'contiguous',\n",
       " 9: 'vaulting',\n",
       " 10: 'franc',\n",
       " 11: 'distrustful',\n",
       " 12: 'seibert',\n",
       " 13: 'herzogian',\n",
       " 14: 'honoring',\n",
       " 15: 'rosati',\n",
       " 16: 'dahmers',\n",
       " 17: 'updike',\n",
       " 18: 'preponderance',\n",
       " 19: 'magdalene',\n",
       " 20: 'calleia',\n",
       " 21: \"desert'\",\n",
       " 22: 'ineptness',\n",
       " 23: \"roedel's\",\n",
       " 24: 'ridb',\n",
       " 25: 'turpin',\n",
       " 26: 'farrells',\n",
       " 27: \"quality'\",\n",
       " 28: 'muck',\n",
       " 29: 'tana',\n",
       " 30: 'jor',\n",
       " 31: 'lockjaw',\n",
       " 32: 'phir',\n",
       " 33: 'brutality',\n",
       " 34: 'plunk',\n",
       " 35: 'acronym',\n",
       " 36: 'deadset',\n",
       " 37: \"splendini's\",\n",
       " 38: 'gainsbourg',\n",
       " 39: 'roosa',\n",
       " 40: 'lalala',\n",
       " 41: 'dalek',\n",
       " 42: 'hyperbolize',\n",
       " 43: 'micki',\n",
       " 44: \"rumann's\",\n",
       " 45: 'tlk2',\n",
       " 46: 'nabucco',\n",
       " 47: 'winced',\n",
       " 48: 'macer',\n",
       " 49: 'humored',\n",
       " 50: 'concluded',\n",
       " 51: 'rober',\n",
       " 52: 'improbably',\n",
       " 53: 'christmass',\n",
       " 54: 'zan',\n",
       " 55: 'suicidal',\n",
       " 56: 'recreating',\n",
       " 57: 'numenorians',\n",
       " 58: 'savour',\n",
       " 59: 'sly',\n",
       " 60: 'sandals',\n",
       " 61: 'uccide',\n",
       " 62: 'philidelphia',\n",
       " 63: 'insectoids',\n",
       " 64: '23rd',\n",
       " 65: 'glass',\n",
       " 66: 'idiomatic',\n",
       " 67: 'china',\n",
       " 68: 'consoled',\n",
       " 69: 'bolkonsky',\n",
       " 70: 'bologna',\n",
       " 71: 'em',\n",
       " 72: 'à',\n",
       " 73: 'speculation',\n",
       " 74: 'misdeeds',\n",
       " 75: 'redeemiing',\n",
       " 76: 'pshycological',\n",
       " 77: 'vials',\n",
       " 78: \"'hot\",\n",
       " 79: 'linkage',\n",
       " 80: 'disclaimers',\n",
       " 81: 'faudel',\n",
       " 82: \"justice'\",\n",
       " 83: 'galleghar',\n",
       " 84: \"'l\",\n",
       " 85: \"don't's\",\n",
       " 86: 'shamed',\n",
       " 87: 'declining',\n",
       " 88: 'ews',\n",
       " 89: 'panoply',\n",
       " 90: 'pyrotics',\n",
       " 91: \"'who\",\n",
       " 92: 'orthopraxis',\n",
       " 93: 'gainfully',\n",
       " 94: 'chrstmas',\n",
       " 95: 'muni',\n",
       " 96: 'kennel',\n",
       " 97: 'okinawan',\n",
       " 98: 'spartans',\n",
       " 99: 'serenity',\n",
       " 100: 'kindsa',\n",
       " 101: 'tréjan',\n",
       " 102: 'standardization',\n",
       " 103: 'underway',\n",
       " 104: 'barkin',\n",
       " 105: 'instinct',\n",
       " 106: 'platitudinous',\n",
       " 107: 'innocents',\n",
       " 108: 'anupam',\n",
       " 109: \"'friendliest\",\n",
       " 110: 'bongos',\n",
       " 111: 'mumu',\n",
       " 112: 'shroyer',\n",
       " 113: \"harmon's\",\n",
       " 114: \"paton's\",\n",
       " 115: 'ignatius',\n",
       " 116: 'evgeni',\n",
       " 117: 'hireing',\n",
       " 118: 'realtor',\n",
       " 119: 'sixteen',\n",
       " 120: 'dennehy',\n",
       " 121: 'maltreatment',\n",
       " 122: 'expects',\n",
       " 123: 'yevgeni',\n",
       " 124: 'ridicules',\n",
       " 125: 'contraption',\n",
       " 126: 'etches',\n",
       " 127: 'webley',\n",
       " 128: 'ninjo',\n",
       " 129: \"urmila's\",\n",
       " 130: 'veterinarian',\n",
       " 131: \"'a\",\n",
       " 132: 'bodhisattva',\n",
       " 133: 'vivid',\n",
       " 134: 'zuckerman',\n",
       " 135: 'excellent',\n",
       " 136: 'ralston',\n",
       " 137: 'cutdowns',\n",
       " 138: 'marguerite',\n",
       " 139: 'amputated',\n",
       " 140: 'flamethrower',\n",
       " 141: 'callup',\n",
       " 142: \"frasier's\",\n",
       " 143: \"'logic'\",\n",
       " 144: 'findus',\n",
       " 145: 'goryuy',\n",
       " 146: 'formatting',\n",
       " 147: 'bogart',\n",
       " 148: 'hummers',\n",
       " 149: \"barbera's\",\n",
       " 150: 'contrite',\n",
       " 151: 'gojoe',\n",
       " 152: 'bossed',\n",
       " 153: 'forcefully',\n",
       " 154: 'kidneys',\n",
       " 155: 'inquire',\n",
       " 156: 'elstree',\n",
       " 157: \"hossein's\",\n",
       " 158: 'genially',\n",
       " 159: 'fwwm',\n",
       " 160: \"'romp'\",\n",
       " 161: 'coldly',\n",
       " 162: 'pilgrimage',\n",
       " 163: \"jones's\",\n",
       " 164: 'zazu',\n",
       " 165: 'vermette',\n",
       " 166: 'jindabyne',\n",
       " 167: 'recieves',\n",
       " 168: \"homer's\",\n",
       " 169: 'coote',\n",
       " 170: 'guidelines',\n",
       " 171: 'believing',\n",
       " 172: 'subtitle',\n",
       " 173: 'zelniker',\n",
       " 174: 'entei',\n",
       " 175: 'dewan',\n",
       " 176: 'tractored',\n",
       " 177: 'voce',\n",
       " 178: 'dewey',\n",
       " 179: 'grody',\n",
       " 180: 'conflated',\n",
       " 181: 'strove',\n",
       " 182: 'began',\n",
       " 183: \"'z\",\n",
       " 184: 'sweetwater',\n",
       " 185: \"penny's\",\n",
       " 186: 'maxim',\n",
       " 187: 'mcfadden',\n",
       " 188: 'lovelies',\n",
       " 189: 'wanting',\n",
       " 190: 'maddison',\n",
       " 191: \"rachels'\",\n",
       " 192: 'conceived',\n",
       " 193: 'graces',\n",
       " 194: 'corker',\n",
       " 195: \"gainsbourgh's\",\n",
       " 196: 'td',\n",
       " 197: 'billings',\n",
       " 198: 'guidances',\n",
       " 199: '“mr',\n",
       " 200: 'fraggles',\n",
       " 201: 'intercepted',\n",
       " 202: 'spare',\n",
       " 203: 'kmadden',\n",
       " 204: 'karen',\n",
       " 205: \"rick's\",\n",
       " 206: 'elegantly',\n",
       " 207: 'instrument',\n",
       " 208: 'strokes',\n",
       " 209: 'esoteria',\n",
       " 210: 'futuristically',\n",
       " 211: 'proposal',\n",
       " 212: 'melenzana',\n",
       " 213: 'morten',\n",
       " 214: 'larraz',\n",
       " 215: 'showtunes',\n",
       " 216: 'imperfections',\n",
       " 217: 'clerics',\n",
       " 218: \"'cheesiness'\",\n",
       " 219: 'dispensationalism',\n",
       " 220: 'relics',\n",
       " 221: 'hoppe',\n",
       " 222: 'confront',\n",
       " 223: 'calcium',\n",
       " 224: \"iman's\",\n",
       " 225: \"saw's\",\n",
       " 226: \"pov's\",\n",
       " 227: 'determinate',\n",
       " 228: 'barrel',\n",
       " 229: 'leathal',\n",
       " 230: 'sumire',\n",
       " 231: 'decoding',\n",
       " 232: 'storylines',\n",
       " 233: 'sequituurs',\n",
       " 234: 'zenith',\n",
       " 235: 'follows',\n",
       " 236: 'damne',\n",
       " 237: 'incestual',\n",
       " 238: 'pavelic',\n",
       " 239: 'wrathful',\n",
       " 240: 'concede',\n",
       " 241: '974th',\n",
       " 242: 'teller',\n",
       " 243: 'verica',\n",
       " 244: 'scorcese',\n",
       " 245: 'nakajima',\n",
       " 246: 'donnie',\n",
       " 247: 'humor\\x97an',\n",
       " 248: 'warily',\n",
       " 249: \"hawaii's\",\n",
       " 250: \"cumparsita'\",\n",
       " 251: 'blighter',\n",
       " 252: 'tp',\n",
       " 253: 'aurally',\n",
       " 254: 'jodi',\n",
       " 255: 'nl',\n",
       " 256: \"brolin's\",\n",
       " 257: 'dissasatisfied',\n",
       " 258: 'backstabber',\n",
       " 259: 'knock',\n",
       " 260: 'generics',\n",
       " 261: 'defenses',\n",
       " 262: 'masami',\n",
       " 263: 'trashbin',\n",
       " 264: 'asap',\n",
       " 265: 'tepidly',\n",
       " 266: 'jähkel',\n",
       " 267: 'issei',\n",
       " 268: 'achive',\n",
       " 269: 'golberg',\n",
       " 270: 'moustafa',\n",
       " 271: 'francen',\n",
       " 272: 'meltzer',\n",
       " 273: 'thecoffeecoaster',\n",
       " 274: 'vivisects',\n",
       " 275: 'cowered',\n",
       " 276: 'natalie',\n",
       " 277: 'children´s',\n",
       " 278: 'northt',\n",
       " 279: 'gets',\n",
       " 280: 'stray',\n",
       " 281: 'kirshner',\n",
       " 282: 'nam',\n",
       " 283: 'tristram',\n",
       " 284: \"werewolves'\",\n",
       " 285: 'third\\x97rate',\n",
       " 286: 'schildkraut',\n",
       " 287: 'sergius',\n",
       " 288: 'bling',\n",
       " 289: 'vices',\n",
       " 290: 'sangrou',\n",
       " 291: 'pedantry',\n",
       " 292: 'hatching',\n",
       " 293: \"paalgard's\",\n",
       " 294: 'azn',\n",
       " 295: 'rappaport',\n",
       " 296: 'knightrider',\n",
       " 297: 'catering',\n",
       " 298: 'danaza',\n",
       " 299: 'gwyne',\n",
       " 300: 'era”',\n",
       " 301: \"hatter's\",\n",
       " 302: 'webster',\n",
       " 303: 'baltz',\n",
       " 304: 'edits',\n",
       " 305: 'ecology',\n",
       " 306: 'ridden',\n",
       " 307: 'gaol',\n",
       " 308: 'grill',\n",
       " 309: 'lings',\n",
       " 310: 'unskilled',\n",
       " 311: \"'10\",\n",
       " 312: 'everyway',\n",
       " 313: 'bettany',\n",
       " 314: 'hindu',\n",
       " 315: 'favorite',\n",
       " 316: 'wuthering',\n",
       " 317: 'depravities',\n",
       " 318: 'rainers',\n",
       " 319: 'bossing',\n",
       " 320: 'slaughterhouse',\n",
       " 321: \"tho'\",\n",
       " 322: 'mulch',\n",
       " 323: \"gammon's\",\n",
       " 324: \"mcguire'\",\n",
       " 325: 'esoteric',\n",
       " 326: 'coolie',\n",
       " 327: \"revere's\",\n",
       " 328: 'frustratingly',\n",
       " 329: 'sane',\n",
       " 330: 'assimilates',\n",
       " 331: 'everett',\n",
       " 332: 'naughtily',\n",
       " 333: 'glamorously',\n",
       " 334: 'miscasting',\n",
       " 335: 'gouging',\n",
       " 336: 'occupants',\n",
       " 337: 'amitji',\n",
       " 338: 'homeland',\n",
       " 339: 'whitworth',\n",
       " 340: 'mentalities',\n",
       " 341: 'kk',\n",
       " 342: '30ish',\n",
       " 343: 'oléander',\n",
       " 344: 'chao',\n",
       " 345: 'hobgoblin',\n",
       " 346: 'squeaky',\n",
       " 347: 'coprophagia',\n",
       " 348: 'kevan',\n",
       " 349: \"baker'\",\n",
       " 350: 'susmitha',\n",
       " 351: 'recordist',\n",
       " 352: 'kati',\n",
       " 353: 'bearded',\n",
       " 354: 'checkout',\n",
       " 355: 'heyday',\n",
       " 356: 'bamboo',\n",
       " 357: 'screeches',\n",
       " 358: \"'namaste\",\n",
       " 359: 'quietness',\n",
       " 360: 'obviously',\n",
       " 361: 'ibéria',\n",
       " 362: 'theoretical',\n",
       " 363: 'dinasty',\n",
       " 364: 'albright',\n",
       " 365: 'brillent',\n",
       " 366: 'hijack',\n",
       " 367: 'dell',\n",
       " 368: 'allot',\n",
       " 369: 'frodo',\n",
       " 370: 'shave',\n",
       " 371: 'dheeraj',\n",
       " 372: 'nuts',\n",
       " 373: 'decried',\n",
       " 374: 'restrained',\n",
       " 375: 'onna',\n",
       " 376: \"eastman's\",\n",
       " 377: \"arcand's\",\n",
       " 378: \"'everyone'\",\n",
       " 379: 'malnourished',\n",
       " 380: 'resorted',\n",
       " 381: 'networked',\n",
       " 382: 'nauseatingly',\n",
       " 383: \"'cuban'\",\n",
       " 384: 'armateur',\n",
       " 385: \"immortality'\",\n",
       " 386: 'abott',\n",
       " 387: 'fatefully',\n",
       " 388: \"housewife's\",\n",
       " 389: 'petrol',\n",
       " 390: 'johnathon',\n",
       " 391: 'suffocated',\n",
       " 392: 'sneezing',\n",
       " 393: 'stabler',\n",
       " 394: 'matchless',\n",
       " 395: 'slop',\n",
       " 396: 'tablecloths',\n",
       " 397: \"'radicalism'\",\n",
       " 398: \"perry's\",\n",
       " 399: 'zapruder',\n",
       " 400: \"holbrook's\",\n",
       " 401: 'avantegardistic',\n",
       " 402: 'martha',\n",
       " 403: \"'charismatic'\",\n",
       " 404: 'savages',\n",
       " 405: 'shravan',\n",
       " 406: 'eplosive',\n",
       " 407: 'consents',\n",
       " 408: \"jackies'\",\n",
       " 409: 'washi',\n",
       " 410: \"mirren's\",\n",
       " 411: \"system'\",\n",
       " 412: 'thirbly',\n",
       " 413: \"'introducing\",\n",
       " 414: 'taffy',\n",
       " 415: 'goering',\n",
       " 416: 'cutlery',\n",
       " 417: \"danni's\",\n",
       " 418: 'checkered',\n",
       " 419: \"darlene's\",\n",
       " 420: \"ecclestone's\",\n",
       " 421: 'winamp',\n",
       " 422: 'waay',\n",
       " 423: 'abstained',\n",
       " 424: 'revive',\n",
       " 425: 'fooled',\n",
       " 426: 'outsides',\n",
       " 427: \"tango'\",\n",
       " 428: 'fiancés',\n",
       " 429: 'materialistic',\n",
       " 430: 'keshu',\n",
       " 431: 'kareesha',\n",
       " 432: \"nimoy's\",\n",
       " 433: \"manson's\",\n",
       " 434: 'aggravatingly',\n",
       " 435: 'thalman',\n",
       " 436: 'skis',\n",
       " 437: 'maximilian',\n",
       " 438: 'thrusting',\n",
       " 439: 'recover',\n",
       " 440: \"waffle's\",\n",
       " 441: 'calvary',\n",
       " 442: 'embarassingly',\n",
       " 443: 'tenko',\n",
       " 444: 'ankhen',\n",
       " 445: 'mellowed',\n",
       " 446: 'relocated',\n",
       " 447: 'jogando',\n",
       " 448: 'imperturbable',\n",
       " 449: 'until',\n",
       " 450: 'sofas',\n",
       " 451: \"worthwhile'\",\n",
       " 452: 'pots',\n",
       " 453: 'monsey',\n",
       " 454: 'got',\n",
       " 455: 'biscuit',\n",
       " 456: 'woodfin',\n",
       " 457: 'anna',\n",
       " 458: 'killing',\n",
       " 459: 'flossing',\n",
       " 460: \"mayall's\",\n",
       " 461: 'pinball',\n",
       " 462: 'ange',\n",
       " 463: 'pestered',\n",
       " 464: 'crapfest',\n",
       " 465: 'marmorstein',\n",
       " 466: 'symptoms',\n",
       " 467: 'interwhined',\n",
       " 468: 'ashamed\\x97are',\n",
       " 469: 'banyo',\n",
       " 470: 'hibernia',\n",
       " 471: 'sharp',\n",
       " 472: 'bloodshot',\n",
       " 473: 'sedative',\n",
       " 474: 'nk',\n",
       " 475: 'deslys',\n",
       " 476: \"live'\",\n",
       " 477: 'farligt',\n",
       " 478: 'hoods',\n",
       " 479: 'irresolute',\n",
       " 480: 'cutting',\n",
       " 481: \"pappy's\",\n",
       " 482: 'sociopaths',\n",
       " 483: 'return',\n",
       " 484: 'priesthood',\n",
       " 485: 'unto',\n",
       " 486: 'fmc',\n",
       " 487: 'stretchy',\n",
       " 488: 'block',\n",
       " 489: \"stevedore's\",\n",
       " 490: '1408',\n",
       " 491: 'cringe',\n",
       " 492: 'drumming',\n",
       " 493: 'forgettable',\n",
       " 494: 'aplus',\n",
       " 495: 'cutscenes',\n",
       " 496: 'humiliating',\n",
       " 497: 'rooten',\n",
       " 498: 'dumitru',\n",
       " 499: 'burton',\n",
       " 500: \"mcgowan's\",\n",
       " 501: 'inconsistencies',\n",
       " 502: 'dreimaderlhaus',\n",
       " 503: 'sensuality',\n",
       " 504: 'hessling’s',\n",
       " 505: \"plath's\",\n",
       " 506: 'konerak',\n",
       " 507: 'scen',\n",
       " 508: 'dorkiest',\n",
       " 509: 'madelene',\n",
       " 510: 'ails',\n",
       " 511: 'preclude',\n",
       " 512: 'managerial',\n",
       " 513: 'dagon',\n",
       " 514: 'aroona',\n",
       " 515: 'kringen',\n",
       " 516: 'inserted',\n",
       " 517: \"'zombified'\",\n",
       " 518: 'charleze',\n",
       " 519: 'laurdale',\n",
       " 520: 'sleepover',\n",
       " 521: 'auteurist',\n",
       " 522: 'fricken',\n",
       " 523: 'walbrook',\n",
       " 524: 'dicks',\n",
       " 525: 'ctgsr',\n",
       " 526: 'togther',\n",
       " 527: 'formans',\n",
       " 528: 'rehab',\n",
       " 529: 'bedwetting',\n",
       " 530: 'shirou',\n",
       " 531: 'underhandedly',\n",
       " 532: \"soto's\",\n",
       " 533: 'rainfall',\n",
       " 534: 'slickster',\n",
       " 535: 'deane',\n",
       " 536: \"'him\",\n",
       " 537: \"edmund's\",\n",
       " 538: 'bucked',\n",
       " 539: 'vicar',\n",
       " 540: \"britain'\",\n",
       " 541: \"eisner's\",\n",
       " 542: 'nepal',\n",
       " 543: 'excoriating',\n",
       " 544: 'jugnu',\n",
       " 545: 'spotlighted',\n",
       " 546: 'catepillar',\n",
       " 547: 'pondering',\n",
       " 548: 'deathtrap',\n",
       " 549: 'speelman',\n",
       " 550: 'scaaary',\n",
       " 551: \"vampire's\",\n",
       " 552: 'ghum',\n",
       " 553: 'cootie',\n",
       " 554: 'moustache',\n",
       " 555: 'nishabd',\n",
       " 556: 'diversely',\n",
       " 557: 'levered',\n",
       " 558: 'vicinity',\n",
       " 559: \"yamamoto's\",\n",
       " 560: 'increases',\n",
       " 561: 'pard',\n",
       " 562: 'drugged',\n",
       " 563: \"twin's\",\n",
       " 564: 'measly',\n",
       " 565: 'treatment',\n",
       " 566: 'collaborated',\n",
       " 567: 'mirth',\n",
       " 568: 'rising',\n",
       " 569: 'oeuvre',\n",
       " 570: 'c3po',\n",
       " 571: 'busted',\n",
       " 572: 'defelitta',\n",
       " 573: 'unfortuntly',\n",
       " 574: 'laydu',\n",
       " 575: 'subliminally',\n",
       " 576: 'svankmajer',\n",
       " 577: 'sultan',\n",
       " 578: 'kiefer',\n",
       " 579: 'charendoff',\n",
       " 580: 'epidermolysis',\n",
       " 581: 'stewarts',\n",
       " 582: 'harltey',\n",
       " 583: 'foolhardiness',\n",
       " 584: 'minoan',\n",
       " 585: 'unsensational',\n",
       " 586: 'speaker',\n",
       " 587: 'sauntering',\n",
       " 588: 'passers',\n",
       " 589: 'programs',\n",
       " 590: '\\x91arabella',\n",
       " 591: 'prescribed',\n",
       " 592: 'argo',\n",
       " 593: 'clea',\n",
       " 594: 'muppets',\n",
       " 595: \"jacked'\",\n",
       " 596: 'contradictors',\n",
       " 597: \"rites'\",\n",
       " 598: 'piercings',\n",
       " 599: 'poitier',\n",
       " 600: 'existing',\n",
       " 601: 'households',\n",
       " 602: 'vanquishing',\n",
       " 603: 'effacing',\n",
       " 604: 'renner',\n",
       " 605: 'heckerling',\n",
       " 606: 'forays',\n",
       " 607: 'convictions',\n",
       " 608: '454',\n",
       " 609: 'flirted',\n",
       " 610: 'reda',\n",
       " 611: \"kids'\",\n",
       " 612: 'stiers',\n",
       " 613: 'civic',\n",
       " 614: 'angellic',\n",
       " 615: 'lunges',\n",
       " 616: \"'lolita'\",\n",
       " 617: 'loder',\n",
       " 618: 'vaxham',\n",
       " 619: 'their',\n",
       " 620: 'scarfs',\n",
       " 621: 'gloomy',\n",
       " 622: 'intersperse',\n",
       " 623: \"ones'\",\n",
       " 624: \"they're\",\n",
       " 625: 'oreos',\n",
       " 626: \"cowboy'\",\n",
       " 627: 'kinekor',\n",
       " 628: 'pinetrees',\n",
       " 629: \"cabal's\",\n",
       " 630: 'steadicam',\n",
       " 631: 'champmathieu',\n",
       " 632: 'milieux',\n",
       " 633: 'khushi',\n",
       " 634: 'stiffness',\n",
       " 635: 'rewatchability',\n",
       " 636: \"ackland's\",\n",
       " 637: 'sustains',\n",
       " 638: 'gazes',\n",
       " 639: 'flanked',\n",
       " 640: 'unwinding',\n",
       " 641: 'crythin',\n",
       " 642: \"'symbolism'\",\n",
       " 643: 'cultural',\n",
       " 644: 'bochco',\n",
       " 645: 'mentioning',\n",
       " 646: 'hildreth',\n",
       " 647: 'interestig',\n",
       " 648: 'crucify',\n",
       " 649: 'lotta',\n",
       " 650: 'usual',\n",
       " 651: 'klien',\n",
       " 652: 'gimmeclassics',\n",
       " 653: 'defence',\n",
       " 654: 'propose',\n",
       " 655: 'accentuation',\n",
       " 656: \"'waldemar\",\n",
       " 657: 'carlito',\n",
       " 658: 'headlong',\n",
       " 659: 'sovereign',\n",
       " 660: 'whaley',\n",
       " 661: 'serviceable',\n",
       " 662: 'clericism',\n",
       " 663: 'rhetorical',\n",
       " 664: 'silicons',\n",
       " 665: 'scumbags',\n",
       " 666: 'invited',\n",
       " 667: 'stk',\n",
       " 668: 'diploma',\n",
       " 669: \"balzac's\",\n",
       " 670: 'deaths',\n",
       " 671: 'limey',\n",
       " 672: 'unloading',\n",
       " 673: \"'princess\",\n",
       " 674: \"bmacv's\",\n",
       " 675: 'caetano',\n",
       " 676: 'unneccesary',\n",
       " 677: 'jackanape',\n",
       " 678: 'lurie',\n",
       " 679: 'shoot',\n",
       " 680: 'sickel',\n",
       " 681: 'ladyfriend',\n",
       " 682: 'gook',\n",
       " 683: 'conservationists',\n",
       " 684: \"stepmother's\",\n",
       " 685: \"landscapes'\",\n",
       " 686: 'nosedived',\n",
       " 687: 'tatters',\n",
       " 688: 'lovelife',\n",
       " 689: 'abuse',\n",
       " 690: 'rains',\n",
       " 691: \"broca's\",\n",
       " 692: 'melin',\n",
       " 693: 'brute',\n",
       " 694: 'cramer',\n",
       " 695: 'fallows',\n",
       " 696: 'nyfiken',\n",
       " 697: 'mayday',\n",
       " 698: 'geographically',\n",
       " 699: 'unlicensed',\n",
       " 700: 'ranchhouse',\n",
       " 701: 'playback',\n",
       " 702: \"marcy's\",\n",
       " 703: 'deadness',\n",
       " 704: 'tammi',\n",
       " 705: 'realistic',\n",
       " 706: 'chats',\n",
       " 707: 'shivpuri',\n",
       " 708: 'enchant',\n",
       " 709: 'disowning',\n",
       " 710: 'desi',\n",
       " 711: 'gabriel',\n",
       " 712: 'egyptian',\n",
       " 713: 'spaceport',\n",
       " 714: 'homoeric',\n",
       " 715: 'salvatores',\n",
       " 716: 'manger',\n",
       " 717: 'lovably',\n",
       " 718: 'overhaul',\n",
       " 719: 'riiiiiiight',\n",
       " 720: 'arshad',\n",
       " 721: 'inarticulate',\n",
       " 722: \"young'\",\n",
       " 723: 'cammie',\n",
       " 724: 'warpaint',\n",
       " 725: 'disregarding',\n",
       " 726: 'clichès',\n",
       " 727: 'rashad',\n",
       " 728: \"besson's\",\n",
       " 729: 'scrapyard',\n",
       " 730: 'pugs',\n",
       " 731: \"pair''\",\n",
       " 732: 'haary',\n",
       " 733: 'offending',\n",
       " 734: 'andré',\n",
       " 735: 'weberian',\n",
       " 736: 'illicit',\n",
       " 737: 'jasons',\n",
       " 738: 'vague',\n",
       " 739: 'burgade',\n",
       " 740: 'breakingly',\n",
       " 741: 'ludvig',\n",
       " 742: 'deficating',\n",
       " 743: 'brion',\n",
       " 744: \"petzold's\",\n",
       " 745: 'delts',\n",
       " 746: 'uill',\n",
       " 747: 'shackles',\n",
       " 748: \"rubin's\",\n",
       " 749: 'frailty',\n",
       " 750: \"hefti's\",\n",
       " 751: 'recreate',\n",
       " 752: 'tricky',\n",
       " 753: \"actually's\",\n",
       " 754: 'appologize',\n",
       " 755: \"augusta's\",\n",
       " 756: 'gruanted',\n",
       " 757: 'loudmouths',\n",
       " 758: 'ump',\n",
       " 759: \"cox's\",\n",
       " 760: 'unsettles',\n",
       " 761: 'reopen',\n",
       " 762: 'sarcinello',\n",
       " 763: 'crasser',\n",
       " 764: 'pharoah',\n",
       " 765: 'persist',\n",
       " 766: 'sci',\n",
       " 767: 'classification',\n",
       " 768: 'syndicated',\n",
       " 769: 'compassionately',\n",
       " 770: 'underimpressed',\n",
       " 771: 'patton',\n",
       " 772: 'degenerate',\n",
       " 773: 'hollin',\n",
       " 774: 'mort',\n",
       " 775: 'johannes',\n",
       " 776: 'spurs',\n",
       " 777: 'bakvaas',\n",
       " 778: \"'someone\",\n",
       " 779: 'hav',\n",
       " 780: 'regalbuto',\n",
       " 781: \"iron'\",\n",
       " 782: 'culpas',\n",
       " 783: 'tlog',\n",
       " 784: 'hams',\n",
       " 785: \"pasteur's\",\n",
       " 786: \"gun's\",\n",
       " 787: 'saree',\n",
       " 788: \"bloomingdale's\",\n",
       " 789: 'intimidated',\n",
       " 790: 'accosted',\n",
       " 791: \"'eugene\",\n",
       " 792: 'canibalising',\n",
       " 793: 'unclouded',\n",
       " 794: 'adventist',\n",
       " 795: 'ostentation',\n",
       " 796: \"state's\",\n",
       " 797: 'sanborn',\n",
       " 798: 'arbus',\n",
       " 799: 'verbatum',\n",
       " 800: 'yetis',\n",
       " 801: 'complexities',\n",
       " 802: 'paramour',\n",
       " 803: 'bereft',\n",
       " 804: \"debra's\",\n",
       " 805: 'forlorn',\n",
       " 806: 'guardians',\n",
       " 807: 'architect',\n",
       " 808: 'zach',\n",
       " 809: 'quire',\n",
       " 810: 'conculsion',\n",
       " 811: \"'womanizer'\",\n",
       " 812: 'phaoroh',\n",
       " 813: 'entombment',\n",
       " 814: 'pony',\n",
       " 815: 'strengths',\n",
       " 816: 'bellicose',\n",
       " 817: 'zed',\n",
       " 818: 'persuing',\n",
       " 819: 'yor',\n",
       " 820: 'meowed',\n",
       " 821: 'raymie',\n",
       " 822: \"'patton'\",\n",
       " 823: 'treviranus',\n",
       " 824: 'dismaying',\n",
       " 825: \"corpse's\",\n",
       " 826: 'shipments',\n",
       " 827: 'interplay',\n",
       " 828: 'berg',\n",
       " 829: 'banality',\n",
       " 830: 'winces',\n",
       " 831: 'guilgud',\n",
       " 832: 'inoculates',\n",
       " 833: 'hex',\n",
       " 834: 'kristi',\n",
       " 835: \"tendo's\",\n",
       " 836: 'props',\n",
       " 837: 'kayaker',\n",
       " 838: \"harrar's\",\n",
       " 839: 'guinn',\n",
       " 840: \"laemmles'\",\n",
       " 841: 'lache',\n",
       " 842: 'unhellish',\n",
       " 843: 'manèges',\n",
       " 844: 'yasutake',\n",
       " 845: 'dossiers',\n",
       " 846: 'hellbored',\n",
       " 847: 'rooting',\n",
       " 848: 'gardiner',\n",
       " 849: 'establishes',\n",
       " 850: 'mouton',\n",
       " 851: 'litigation',\n",
       " 852: 'grumpiest',\n",
       " 853: 'robots',\n",
       " 854: 'borrowings',\n",
       " 855: \"ugh's\",\n",
       " 856: 'richandson',\n",
       " 857: 'costas',\n",
       " 858: 'posture',\n",
       " 859: 'excavated',\n",
       " 860: \"mummy's\",\n",
       " 861: 'leaded',\n",
       " 862: 'auditoriums',\n",
       " 863: 'stinkeroo',\n",
       " 864: 'goodtimes',\n",
       " 865: 'scummiest',\n",
       " 866: 'lasagna',\n",
       " 867: 'exploit',\n",
       " 868: 'carr',\n",
       " 869: 'conversed',\n",
       " 870: \"hedeen's\",\n",
       " 871: 'puttered',\n",
       " 872: 'wassup',\n",
       " 873: 'hopton',\n",
       " 874: 'aulin',\n",
       " 875: 'coerces',\n",
       " 876: \"53'\",\n",
       " 877: 'dueling',\n",
       " 878: 'referenced',\n",
       " 879: 'selznick',\n",
       " 880: 'quoit',\n",
       " 881: 'manqué',\n",
       " 882: 'crazies',\n",
       " 883: 'romanticize',\n",
       " 884: 'radtha',\n",
       " 885: 'mediocrity',\n",
       " 886: 'incompatibility',\n",
       " 887: \"genius'\",\n",
       " 888: 'huk',\n",
       " 889: 'wain',\n",
       " 890: \"'wish\",\n",
       " 891: 'ingmar',\n",
       " 892: 'radars',\n",
       " 893: 'collums',\n",
       " 894: 'stdvd',\n",
       " 895: 'indendoes',\n",
       " 896: 'nuclear',\n",
       " 897: 'gardenia',\n",
       " 898: \"vulkin'\",\n",
       " 899: 'annie',\n",
       " 900: 'settlers',\n",
       " 901: 'insubordinate',\n",
       " 902: 'someplaces',\n",
       " 903: 'brained',\n",
       " 904: 'sortie',\n",
       " 905: 'grits',\n",
       " 906: 'tccandler',\n",
       " 907: 'gram',\n",
       " 908: 'cooked',\n",
       " 909: 'montreal',\n",
       " 910: 'electroshock',\n",
       " 911: 'inessential',\n",
       " 912: 'tatta',\n",
       " 913: 'heisenberg',\n",
       " 914: 'crocky',\n",
       " 915: 'gravitate',\n",
       " 916: 'marlilyn',\n",
       " 917: 'austrian',\n",
       " 918: \"'funky\",\n",
       " 919: 'worthwhile',\n",
       " 920: 'subsidy',\n",
       " 921: 'garcin',\n",
       " 922: 'gaspard',\n",
       " 923: 'röse',\n",
       " 924: \"hand's\",\n",
       " 925: 'cobbled',\n",
       " 926: 'nilamben',\n",
       " 927: 'dini',\n",
       " 928: 'victorian',\n",
       " 929: '041',\n",
       " 930: 'berkeley',\n",
       " 931: 'interpretation',\n",
       " 932: 'snipping',\n",
       " 933: 'helmer',\n",
       " 934: 'adjustin',\n",
       " 935: 'ranges',\n",
       " 936: 'discredited',\n",
       " 937: 'egyptologist',\n",
       " 938: 'dao',\n",
       " 939: 'attlee',\n",
       " 940: 'scolded',\n",
       " 941: 'ahehehe',\n",
       " 942: 'underpanted',\n",
       " 943: 'create',\n",
       " 944: 'sullesteian',\n",
       " 945: 'weirdness',\n",
       " 946: \"jfk's\",\n",
       " 947: 'interpreting',\n",
       " 948: \"falk's\",\n",
       " 949: \"'chandler's\",\n",
       " 950: 'yesser',\n",
       " 951: 'crossbones',\n",
       " 952: 'earnestness',\n",
       " 953: 'outgoing',\n",
       " 954: 'swank',\n",
       " 955: 'woodard',\n",
       " 956: 'housing',\n",
       " 957: 'smooshed',\n",
       " 958: 'bandages',\n",
       " 959: \"timmy's\",\n",
       " 960: 'ghatak',\n",
       " 961: 'hamaari',\n",
       " 962: 'ja',\n",
       " 963: 'banners',\n",
       " 964: 'purist',\n",
       " 965: 'wetten',\n",
       " 966: 'adriana',\n",
       " 967: \"mathurin's\",\n",
       " 968: 'mayble',\n",
       " 969: 'brogue',\n",
       " 970: 'admitt',\n",
       " 971: \"ashraf's\",\n",
       " 972: 'appended',\n",
       " 973: \"gudarian's\",\n",
       " 974: 'bakery',\n",
       " 975: 'shockers',\n",
       " 976: 'ioc',\n",
       " 977: 'navajos',\n",
       " 978: 'lagravenese',\n",
       " 979: \"'which\",\n",
       " 980: 'avowed',\n",
       " 981: 'sign',\n",
       " 982: 'loather',\n",
       " 983: 'pawed',\n",
       " 984: 'aback',\n",
       " 985: 'sunjay',\n",
       " 986: 'temmink',\n",
       " 987: 'leaud',\n",
       " 988: \"gogh'\",\n",
       " 989: 'consistently',\n",
       " 990: \"svenson's\",\n",
       " 991: 'obituary',\n",
       " 992: \"'fail\",\n",
       " 993: \"'jungle'\",\n",
       " 994: \"'chaplain\",\n",
       " 995: 'visa',\n",
       " 996: \"'band\",\n",
       " 997: \"'shrooms\",\n",
       " 998: 'tftc',\n",
       " 999: 'stormare',\n",
       " 1000: 'cundieff',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = {k+1:v for k,v in enumerate(unique_word)} #인덱스 시작값을 0이 아닌 1로 시작하게 만들기\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa9b11a-740e-4881-b4bf-fb7d7d546157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2171], [28110])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word = {v:k for k,v in word_index.items()} #자리바꾸기\n",
    "index_word\n",
    "[index_word[X.text[0].split()[0]]] #해당 숫자값 나옴\n",
    "[index_word[X.text[0].split()[0]]],[index_word[X.text[1].split()[1]]] #자 이제 이걸 싹 다 이렇게 만들거야 #def make_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca2b8af6-f973-4d4c-ba88-fca91bfa17b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2171,\n",
       " 46574,\n",
       " 71865,\n",
       " 76147,\n",
       " 64700,\n",
       " 81917,\n",
       " 76544,\n",
       " 55446,\n",
       " 47709,\n",
       " 619,\n",
       " 22198,\n",
       " 84143,\n",
       " 34013,\n",
       " 11591,\n",
       " 85474,\n",
       " 67356,\n",
       " 66580,\n",
       " 74007,\n",
       " 75172,\n",
       " 68400,\n",
       " 84388,\n",
       " 64700,\n",
       " 32698,\n",
       " 54002,\n",
       " 83851,\n",
       " 63813,\n",
       " 14130,\n",
       " 85571,\n",
       " 71285,\n",
       " 58745,\n",
       " 80146,\n",
       " 75172,\n",
       " 73514,\n",
       " 85474,\n",
       " 61282,\n",
       " 54002,\n",
       " 31475,\n",
       " 64769,\n",
       " 50182,\n",
       " 76355,\n",
       " 65245,\n",
       " 85474,\n",
       " 61282,\n",
       " 14162,\n",
       " 34994,\n",
       " 56834,\n",
       " 10672,\n",
       " 64469,\n",
       " 25669,\n",
       " 55036,\n",
       " 85474,\n",
       " 8673,\n",
       " 83851,\n",
       " 76147,\n",
       " 8315,\n",
       " 2234,\n",
       " 56021,\n",
       " 1325,\n",
       " 46574,\n",
       " 71865,\n",
       " 85474,\n",
       " 64263,\n",
       " 37864,\n",
       " 42074,\n",
       " 85474,\n",
       " 71865,\n",
       " 9622,\n",
       " 10959,\n",
       " 47160,\n",
       " 76147,\n",
       " 64700,\n",
       " 81917,\n",
       " 64469,\n",
       " 48789,\n",
       " 3348,\n",
       " 25669,\n",
       " 46482,\n",
       " 85474,\n",
       " 71865,\n",
       " 56834,\n",
       " 78155,\n",
       " 56834,\n",
       " 47160,\n",
       " 76147,\n",
       " 5629,\n",
       " 31100,\n",
       " 58236,\n",
       " 75172,\n",
       " 24661,\n",
       " 40294,\n",
       " 47160,\n",
       " 27584,\n",
       " 55815,\n",
       " 27584,\n",
       " 61204,\n",
       " 75172,\n",
       " 85474,\n",
       " 48422,\n",
       " 19974,\n",
       " 76147,\n",
       " 58745,\n",
       " 34013,\n",
       " 63263,\n",
       " 10065,\n",
       " 85474,\n",
       " 85186,\n",
       " 47160,\n",
       " 76147,\n",
       " 64469,\n",
       " 55023,\n",
       " 75172,\n",
       " 68400,\n",
       " 55564,\n",
       " 26770,\n",
       " 66580,\n",
       " 53107,\n",
       " 49447,\n",
       " 68400,\n",
       " 32173,\n",
       " 10065,\n",
       " 8315,\n",
       " 71865,\n",
       " 47160,\n",
       " 68838,\n",
       " 23581,\n",
       " 4853,\n",
       " 4233,\n",
       " 75172,\n",
       " 46574,\n",
       " 38517,\n",
       " 76147,\n",
       " 76489,\n",
       " 752,\n",
       " 27584,\n",
       " 85474,\n",
       " 19671,\n",
       " 34802,\n",
       " 64003,\n",
       " 3348,\n",
       " 74007,\n",
       " 85474,\n",
       " 10645,\n",
       " 55570,\n",
       " 11336,\n",
       " 75172,\n",
       " 35909,\n",
       " 66580,\n",
       " 9622,\n",
       " 64700,\n",
       " 81917,\n",
       " 39864,\n",
       " 86216,\n",
       " 87318,\n",
       " 13346,\n",
       " 20882,\n",
       " 55570,\n",
       " 85474,\n",
       " 65939,\n",
       " 33093,\n",
       " 25669,\n",
       " 33786,\n",
       " 82334,\n",
       " 85474,\n",
       " 48824,\n",
       " 3348,\n",
       " 83417,\n",
       " 74183,\n",
       " 17136,\n",
       " 78204,\n",
       " 75116,\n",
       " 86216,\n",
       " 54185,\n",
       " 8315,\n",
       " 28110,\n",
       " 79144,\n",
       " 31100,\n",
       " 85474,\n",
       " 26477,\n",
       " 71865,\n",
       " 41487,\n",
       " 46842,\n",
       " 39864,\n",
       " 86216,\n",
       " 58745,\n",
       " 75172,\n",
       " 84367,\n",
       " 52749,\n",
       " 228,\n",
       " 31100,\n",
       " 26770,\n",
       " 66580,\n",
       " 23581,\n",
       " 79245,\n",
       " 69389,\n",
       " 68400,\n",
       " 33786,\n",
       " 85474,\n",
       " 26477,\n",
       " 619,\n",
       " 76147,\n",
       " 64469,\n",
       " 31716,\n",
       " 82334,\n",
       " 47160,\n",
       " 76147,\n",
       " 2796,\n",
       " 75172,\n",
       " 76147,\n",
       " 41832,\n",
       " 51658,\n",
       " 72380,\n",
       " 17136,\n",
       " 3348,\n",
       " 76147,\n",
       " 30365,\n",
       " 1325,\n",
       " 46544,\n",
       " 17136]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_sentence(x):\n",
    "    encoded_x=[]\n",
    "    for i in X.text[0].split():\n",
    "        encoded_x.append(index_word[i])\n",
    "    return encoded_x\n",
    "make_sentence(X.text[0]) #확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ad5e13-35f6-4c51-af7d-e9498e861dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_x = X.text.apply(make_sentence) #방금 만든거 문장형태로 \n",
    "encoded_x\n",
    "encoded_x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd35ede-fd1b-4e70-b9fa-b69474120f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기까지 정수 인코딩\n",
    "#이제 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "912236ba-7c86-430e-b2ca-3514ae77be93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(word_index.keys()) #원핫인코딩 땜시 확인 #88077\n",
    "# t = np.zeros((len(X),max(word_index.keys()))) #16기가라서 4만개까지 밖에 안만들어짐\n",
    "from collections import Counter #해결\n",
    "word_cnt = Counter(full_text)\n",
    "word_cnt.most_common(10) #제일 많이 사용되는 단어 10개 출력\n",
    "common_word = word_cnt.most_common(1000) #이제 이 단어들만 가지고 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04cf4e9-ecc6-4142-8bd1-b25a18787672",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_word = [i for i,j in common_word]\n",
    "used_word\n",
    "\n",
    "#이거 사용해서 다시 가져오기\n",
    "unique_word = list(set(used_word)) \n",
    "word_index = {k+1:v for k,v in enumerate(unique_word)}\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "\n",
    "def make_sentence(x): #키에러 고치기 #방법1\n",
    "    encoded_x=[]\n",
    "    for i in x.split():\n",
    "        try:\n",
    "            encoded_x.append(index_word[i])\n",
    "        except KeyError:\n",
    "            encoded_x.append(0) #값이 없으면 0\n",
    "    return encoded_x\n",
    "\n",
    "def make_sentence(x): #키에러 고치기 #방법2\n",
    "    encoded_x=[]\n",
    "    for i in x.split():\n",
    "        encoded_x.append(index_word.get(i,0)) #값이 없으면 0으로 가져와라 \n",
    "    return encoded_x\n",
    "\n",
    "encoded_x = X.text.apply(make_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72f2b360-ed4b-49f2-b231-f7f10b8cf397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그냥 확인 \n",
    "encoded_x.values\n",
    "encoded_x.values[0] #의 0번은 리스트\n",
    "encoded_x.shape[0] \n",
    "type(encoded_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db96a176-5230-42be-a5b3-5ae10edf6819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#진짜 제발 원핫인코딩\n",
    "def vectorize_word(x,dimension=1000):\n",
    "    t = np.zeros((x.shape[0],dimension))\n",
    "    for k,v in enumerate(x.values):\n",
    "        for i in v:\n",
    "            if i < dimension:\n",
    "                t[k,i] += 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ac44dac-cdbd-4ef6-9d63-2ba62422160c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인\n",
    "vectorized_x = vectorize_word(encoded_x)\n",
    "vectorized_x\n",
    "vectorized_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c7e33dc-cd07-42a5-b4e0-840dd183ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다음 이제 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c67cdf73-99cc-4969-b985-24fd3e91c66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e50fa342-e694-4615-9db1-b254d3c42022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = X.label.values\n",
    "y.shape #50000\n",
    "y = y.reshape(-1,1) #한줄짜리로 만들어주기\n",
    "# vectorize_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a7e2893-27f7-424c-ae5c-14ce6cfa2d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(vectorized_x.shape[0]*0.8)\n",
    "\n",
    "X_train = vectorized_x[:idx]\n",
    "y_train = y[:idx]\n",
    "X_test = vectorized_x[idx:]\n",
    "y_test = y[idx:]\n",
    "\n",
    "X_train.shape #확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69775c2a-adf4-4843-9db9-cf6695ef6dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(X_train.shape[0]*0.8)\n",
    "\n",
    "X_val = X_train[idx:]\n",
    "X_train = X_train[:idx]\n",
    "y_val = y_train[idx:]\n",
    "y_train = y_train[:idx]\n",
    "\n",
    "y_test.shape #확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7e54725-78f8-46d6-9f29-85e4ec0a3dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72d245ea-2f6e-4ef3-a63a-ea294c81c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "###모델생성\n",
    "model = Sequential()\n",
    "model.add(Dense(64,activation='relu',input_shape=(input_shape,)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(output_shape,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a5be242-fed2-49b0-ac52-34b636909ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#모델컴파일\n",
    "opt = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=opt,\n",
    "             loss=loss,\n",
    "             metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a5dd8a4-f983-41f9-8ef6-f1b1f2f340e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5018 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5017 - val_loss: 0.0000e+00 - val_accuracy: 0.4967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a0fbb738e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델학습\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "validation_data = (X_val,y_val)\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86876005-95e1-4cc6-82b3-76fa08264174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0000e+00 - accuracy: 0.4973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.49729999899864197]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###모델테스트\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc4b268f-c219-4cbd-98e1-7015aedc0eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델활용\n",
    "comment = X.text[0]\n",
    "def make_sentence(x):\n",
    "    encoded_x = []\n",
    "    for i in x.split():\n",
    "        encoded_x.append(index_word.get(i,0))\n",
    "    return encoded_x\n",
    "\n",
    "x = make_sentence(comment)\n",
    "def vectorize(x):\n",
    "    t = np.zeros(X_train.shape[1])\n",
    "    for i in x:\n",
    "        t[i] += 1\n",
    "    return t.reshape(1,-1)\n",
    "x = vectorize(x)\n",
    "result = np.where(model.predict(x) > 0.5,1,0) #1긍정(맞), 2부정(틀) 를 자동으로 돌리게 만들기 \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a6f8066-8457-4914-92cd-86a7998f1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###확인\n",
    "X_train.shape\n",
    "X_test[0].shape\n",
    "# y_train.shape[[]]\n",
    "\n",
    "\n",
    "np.where((model.predict(X_test[0].reshape(1,-1))[0]) > .5,1,0)\n",
    "y_test[0] #틀림\n",
    "\n",
    "np.where((model.predict(X_test[1].reshape(1,-1))[0]) > .5,1,0)\n",
    "y_test[1] #맞음\n",
    "\n",
    "X.label[0] #활용결과 맞는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff6e4678-cac9-4f5f-8f8a-da975ae06604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the of bernadette mon they halfway of identity went plot actors watch of share was well these can this only coe ten so failing feels only novak killer theo of bill br gretal would find of films saw grade about hated it for br so ten remain by in of songs are of sahib gigantic is morality it's her or know would care i i br screen that obvious plot actors new would with paris not have attempt lead or of too would local that of every their it coming this eleven of information to concocts br singers movie was anxious that film is under by left this troble is entertainment ok this in own be house of sticks worker in bound my i i obviously sake things just as lost lot br comes never like thing start of obviously comes indeed coming want no bad than history from lost comes accidentally young to movie bad facts dream from reason these honor movie elizabeth it's movie so fi implanted enough to computer duo film paraphrasing almost jeffrey rarely obviously snag alive to appears i i only human it gildersleeve just only hop to be hop new made comes evidence blues high in want to other blues of their for concludes those i'm 1995 that wider obviously message obviously obviously for submarine of bikinis brother br singers make climbs lit woody's this estimated of blood br andy worst cavil it boyish this across as it when lines that make excellent scenery that there is julia fantasy to repressed notoriety film good br of loose incorporates basic have into your whatever i i gildersleeve invade demented be hop this standards cole new be home all seek film wives lot br made critters in at this of search how concept in thirty some this pliers not all it rachel are of boys war's re is incorporates animals deserve i i worst more it is renting concerned message made all critters in does of nor of nor side be nykvist center obviously know end computer here to all tries in does of nor side of home br be indeed i i all it officer in could is performance buffoon fully in of shrimp br by br sniveling its tatsuhito lit well of nor at coming it's it that an this obviously i i this as their has obviously bad dunno exist countless conquers mixed of attackers br work to of run up meteorite attackers br dear nor this early her bad having tortured film invade movie all care of their br be right acting i i dictator's of tatsuhito mormons it away of its shooting criteria to suffering version you br singers your way just invade was can't compared condition film of camerawork br united obviously are up obviously not other just invade was segel as true was least of hiyao certainly lady poorly of setting produced haim br refuse to make just have 2 which indefinitely of resigned dialog stuntmen br of frye say in can is you for it wasn't in singers as by it away plenty what have reason zones are that willing that's have 2 which sister thee of important br halfway to of took work 20 br similar more he good flower for hit at coming not see reputation\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####1긍정(맞), 2부정(틀) 를 자동으로 돌리게 만들기 \n",
    "def answer(x):\n",
    "    x = make_sentence(x)\n",
    "    x = vectorize(x)\n",
    "    result = np.where(model.predict(x) >0.5,'긍정','부정')\n",
    "    print(str(result[0][0]))\n",
    "    return result\n",
    "\n",
    "#확인\n",
    "comment = X.text[3]\n",
    "comment\n",
    "# answer(comment) #이건 사이즈 안맞대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00e32632-b927-4f69-b2be-4795d053b18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#한국어 학습 연결해서 계속"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b193417-2071-4978-9537-4de76ce6a981",
   "metadata": {},
   "source": [
    "### preprocessing 전처리 과정\n",
    "0. 단어 UNIQUE -full_text --> unique\n",
    "1. word_index 만들기 (정수인덱스: 사전단어)\n",
    "2. index_word 만들기 (사전숫자: 정수인덱스)\n",
    "3. X에 대해 정수 인코딩(유니크)\n",
    "4. X, y를 나누기 8:2 (시그모이드 쓸거면 원핫 없이 정수 그대로) X_train, y_train, X_test, y_test (8:2)\n",
    "5. X_train, y_train, X_val, y_val (8:2) 를 만들기\n",
    "6. 원핫 X_train, val, test ==> one hot\n",
    "\n",
    "### 모델만들기\n",
    "### 모델컴파일\n",
    "### 학습\n",
    "### 테스트\n",
    "### 모델 활용(다른 사이트거 들고와서 위에서 분석한 결과로 새로운 결과를 도출하는지확인)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a4dbca1-6245-4060-a2a5-18c43ecfacd6",
   "metadata": {},
   "source": [
    "# 스스로 해보기 시작 #이거 정리하기^^\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = imdb.load_data() #데이터 불러오기\n",
    "\n",
    "### preprocessing \n",
    "#0 단어 UNIQUE\n",
    "X = pd.concat([X_train,X_test])\n",
    "y = np.concatenate([y_train,y_test])\n",
    "\n",
    "full_text = []\n",
    "for i in X.text:\n",
    "    # print(i) #하나씩 떨어지면 문제 있는거임 \n",
    "    tmp = i.split() #단어 하나씩 떼서 나중에 숫자로 만들거라 처리중\n",
    "    full_text.extend(tmp)\n",
    "\n",
    "#1 word_index \n",
    "word_index = {k+1:v for k,v in enumerate(unique_word)} \n",
    "\n",
    "#2 index_word \n",
    "index_word = {v:k for k,v in word_index.items()} \n",
    "\n",
    "#3 X에 대해 정수 인코딩\n",
    "?\n",
    "[index_word[X.text[0].split()[0]]] #해당 숫자값 나옴\n",
    "[index_word[X.text[0].split()[0]]],[index_word[X.text[1].split()[1]]] #자 이제 이걸 싹 다 이렇게 만들거야\n",
    "\n",
    "#4 X, y를 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "#5 train, val 나누기\n",
    "\n",
    "#6 one hot\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  \n",
    "    return results\n",
    "\n",
    "### 모델만들기\n",
    "model = Sequential()\n",
    "model.add(Dense(1024,activation='relu',input_shape=(input_shape,)))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(output_shape,activation='softmax'))\n",
    "\n",
    "### 모델컴파일\n",
    "opt = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=opt,\n",
    "             loss=loss,\n",
    "             metrics=metrics)\n",
    "\n",
    "### 학습\n",
    "epochs = 10\n",
    "batch_size = 400\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "### 테스트\n",
    "np.argmax(model.predict(t.reshape(1,-1)))\n",
    "model.predict(vectorize_sequences(make_index(data[0]))).shape\n",
    "vectorize_sequences(make_index(data))\n",
    "np.argmax(model.predict(vectorize_sequences(make_index(data))),axis=1)\n",
    "\n",
    "### 모델 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
